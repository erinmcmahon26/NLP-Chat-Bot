{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54969e19",
   "metadata": {},
   "source": [
    "### Loading Libraries and Ingesting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b667aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3b7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install phrasemachine\n",
    "#!pip install nltk\n",
    "#!pip install rake_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d62c9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import phrasemachine\n",
    "import nltk\n",
    "from rake_nltk import Rake\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import ngrams, FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "395083c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erinmcmahon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/erinmcmahon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only need to run once\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8349b619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/erinmcmahon/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11f30f",
   "metadata": {},
   "source": [
    "#### Adding Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1a56d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stop words from nltk\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49eb569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset to remove punctuation\n",
    "def remove_punctuation(in_text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(in_text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "916514b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset to lower case it\n",
    "def lower_case(in_text):\n",
    "    text = in_text.lower()    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ada6b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset to remove tags\n",
    "def remove_tags(in_text):    \n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",in_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42cad8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset to remove special characters and digits\n",
    "def remove_special_chars_and_digits(in_text):\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", in_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ad85d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset to appy Stemming\n",
    "def apply_stemming(in_text):\n",
    "    stemmer=PorterStemmer()\n",
    "    word_list = nltk.word_tokenize(in_text)\n",
    "    output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56d45f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset to apply Lemmatization\n",
    "def apply_lemmatization(in_text):\n",
    "    # Lemmatization\n",
    "    lem = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(in_text)\n",
    "    output = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d8f629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "def remove_stop_words(in_text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(in_text)  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b9c4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase Machine\n",
    "def run_phrase_machine(in_text):\n",
    "    phrases=phrasemachine.get_phrases(in_text)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8189b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Rake Keyword Extractor\n",
    "def run_rake(in_text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(in_text)\n",
    "    rake_phrases= r.get_ranked_phrases()\n",
    "    return rake_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b34a2",
   "metadata": {},
   "source": [
    "Rapid Automatic Keyword Extraction algorithm that drives to determine key phrases in a body of text by analyzing word frequency and co-occurance with other words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4f78616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NLTK Tokenizer\n",
    "def run_nltk_tokenizer(in_text):\n",
    "    tokens=nltk.word_tokenize(in_text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "496cc946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NLTK Sentence Tokenizer\n",
    "def run_nltk_sent_tokenizer(in_corpus):\n",
    "    sents = nltk.sent_tokenize(in_corpus)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "643a2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run word-ngram Tokenizer\n",
    "def run_nltk_tokenizer_word_ngrams(in_text, ngram_size):\n",
    "    n_grams = ngrams(nltk.word_tokenize(in_text), ngram_size)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8de68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Frequ Dist \n",
    "def get_freq_dist(terms):\n",
    "    all_counts = dict()\n",
    "    all_counts = FreqDist(terms)\n",
    "    return all_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c377a",
   "metadata": {},
   "source": [
    "### Maybe add clean data function from Paquita's notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6243377",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv('https://raw.githubusercontent.com/erinmcmahon26/NLP-Chat-Bot/main/EMU_Movie_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f4d883e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7ce181f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                   FileName                                             Review\n",
       "0   EMU_Doc1_TheConjuring3  I must admit that when I sat down to watch the...\n",
       "1   EMU_Doc2_TheConjuring3  While The Conjuring franchise has stood as one...\n",
       "2   EMU_Doc3_TheConjuring3  We’re well into the world and the lore of the ...\n",
       "3   EMU_Doc4_TheConjuring3  James Wan's 2013 feature The Conjuring was som...\n",
       "4   EMU_Doc5_TheConjuring3  Two Conjuring films and several spinoffs estab...\n",
       "5   EMU_Doc6_TheConjuring3  Right from the first movie, James Wan had bigg...\n",
       "6   EMU_Doc7_TheConjuring3  Money is no issue for The Conjuring films. The...\n",
       "7   EMU_Doc8_TheConjuring3  When a film trots out the phrase “based on a t...\n",
       "8   EMU_Doc9_TheConjuring3  The so-called \"Conjuring universe\" is so succe...\n",
       "9  EMU_Doc10_TheConjuring3  I remember seeing James Wan’s The Conjuring fo...>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b71d40ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMU_Doc1_TheConjuring3</td>\n",
       "      <td>I must admit that when I sat down to watch the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EMU_Doc2_TheConjuring3</td>\n",
       "      <td>While The Conjuring franchise has stood as one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EMU_Doc3_TheConjuring3</td>\n",
       "      <td>We’re well into the world and the lore of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMU_Doc4_TheConjuring3</td>\n",
       "      <td>James Wan's 2013 feature The Conjuring was som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EMU_Doc5_TheConjuring3</td>\n",
       "      <td>Two Conjuring films and several spinoffs estab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 FileName                                             Review\n",
       "0  EMU_Doc1_TheConjuring3  I must admit that when I sat down to watch the...\n",
       "1  EMU_Doc2_TheConjuring3  While The Conjuring franchise has stood as one...\n",
       "2  EMU_Doc3_TheConjuring3  We’re well into the world and the lore of the ...\n",
       "3  EMU_Doc4_TheConjuring3  James Wan's 2013 feature The Conjuring was som...\n",
       "4  EMU_Doc5_TheConjuring3  Two Conjuring films and several spinoffs estab..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c67cf8",
   "metadata": {},
   "source": [
    "### EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3190e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a consolidated clean data function + tokenization\n",
    "\n",
    "def clean_text_tokenize(document):\n",
    "    remove_punc_text = remove_punctuation(document)\n",
    "    lower_text =lower_case(remove_punc_text)\n",
    "    remove_tag_text = remove_tags(lower_text)\n",
    "    remove_special_chars_text = remove_special_chars_and_digits(remove_tag_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = run_nltk_tokenizer(remove_special_chars_text)\n",
    "    tokens = [w for w in word_tokens if not w in stop_words]\n",
    "    tokens = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            tokens.append(w)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548480f1",
   "metadata": {},
   "source": [
    "Below we are applying all preprocessing/data cleaning functions to the movie reviews and separating each review into its own document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a04e344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['tokens'] = text_df['Review'].apply(lambda x: clean_text_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da36e7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [must, admit, sat, watch, addition, conjuring,...\n",
       "1    [conjuring, franchise, stood, one, successful,...\n",
       "2    [well, world, lore, warrens, ed, lorraine, fic...\n",
       "3    [james, wan, feature, conjuring, something, sp...\n",
       "4    [two, conjuring, films, several, spinoffs, est...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c0ce921",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = text_df['tokens'].values[0]\n",
    "doc2 = text_df['tokens'].values[1]\n",
    "doc3 = text_df['tokens'].values[2]\n",
    "doc4 = text_df['tokens'].values[3]\n",
    "doc5 = text_df['tokens'].values[4]\n",
    "doc6 = text_df['tokens'].values[5]\n",
    "doc7 = text_df['tokens'].values[6]\n",
    "doc8 = text_df['tokens'].values[7]\n",
    "doc9 = text_df['tokens'].values[8]\n",
    "doc10 = text_df['tokens'].values[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be94da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "<class 'list'>\n",
      "159\n"
     ]
    }
   ],
   "source": [
    "print(len(doc1))\n",
    "print(type(doc1))\n",
    "print(len(set(doc1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c42c45c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 12),\n",
       " ('conjuring', 11),\n",
       " ('devil', 10),\n",
       " ('made', 10),\n",
       " ('watch', 4),\n",
       " ('horror', 4),\n",
       " ('good', 4),\n",
       " ('franchise', 3),\n",
       " ('say', 3),\n",
       " ('experience', 3)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(doc1).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c0740c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = doc1+doc2+doc3+doc4+doc5+doc6+doc7+doc8+doc9+doc10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "225ea628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conjuring', 69),\n",
       " ('film', 57),\n",
       " ('made', 53),\n",
       " ('devil', 52),\n",
       " ('horror', 39),\n",
       " ('movie', 34),\n",
       " ('warrens', 31),\n",
       " ('series', 29),\n",
       " ('chaves', 28),\n",
       " ('franchise', 27)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(all_docs).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f8bd1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = text_df.Review.values[0]\n",
    "document2 = text_df.Review.values[1]\n",
    "document3 = text_df.Review.values[2]\n",
    "document4 = text_df.Review.values[3]\n",
    "document5 = text_df.Review.values[4]\n",
    "document6 = text_df.Review.values[5]\n",
    "document7 = text_df.Review.values[6]\n",
    "document8 = text_df.Review.values[7]\n",
    "document9 = text_df.Review.values[8]\n",
    "document10 = text_df.Review.values[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e7167e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this first to get sentences from text.\n",
    "sentences_1=run_nltk_sent_tokenizer(document1)\n",
    "sentences_2=run_nltk_sent_tokenizer(document2)\n",
    "sentences_3=run_nltk_sent_tokenizer(document3)\n",
    "sentences_4=run_nltk_sent_tokenizer(document4)\n",
    "sentences_5=run_nltk_sent_tokenizer(document5)\n",
    "sentences_6=run_nltk_sent_tokenizer(document6)\n",
    "sentences_7=run_nltk_sent_tokenizer(document7)\n",
    "sentences_8=run_nltk_sent_tokenizer(document8)\n",
    "sentences_9=run_nltk_sent_tokenizer(document9)\n",
    "sentences_10=run_nltk_sent_tokenizer(document10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a06af260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'conjuring': 11, 'devil made': 10, 'movie': 8, 'watch': 4, 'franchise': 3, 'say': 3, 'movies': 3, 'sit': 2, 'course': 2, 'less': 2, ...})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_1 = []\n",
    "for sentence in sentences_1:\n",
    "    terms_1 = terms_1 + run_rake(sentence)\n",
    "\n",
    "fd_1 = get_freq_dist(terms_1)\n",
    "fd_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "744a437f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'film': 10, 'devil made': 6, 'conjuring': 6, 'franchise': 4, 'one': 3, 'focus': 3, 'sense': 3, 'feature': 3, 'conjuring franchise': 2, 'enough': 2, ...})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_2 = []\n",
    "for sentence in sentences_2:\n",
    "    terms_2 = terms_2 + run_rake(sentence)\n",
    "\n",
    "fd_2 = get_freq_dist(terms_2)\n",
    "fd_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cfb11380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'’': 9, 'love': 3, 'devil made': 3, 'warren ’': 3, 'wan': 3, 'franchise': 3, 'well': 2, 'real': 2, 'arne johnson': 2, 'conjuring': 2, ...})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_3 = []\n",
    "for sentence in sentences_3:\n",
    "    terms_3 = terms_3 + run_rake(sentence)\n",
    "\n",
    "fd_3 = get_freq_dist(terms_3)\n",
    "fd_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be6243bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'well': 5, 'conjuring': 4, 'warrens': 4, 'ed': 3, 'devil made': 3, 'lorraine': 3, 'bring': 3, 'see': 3, 'one': 3, 'real': 2, ...})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_4 = []\n",
    "for sentence in sentences_4:\n",
    "    terms_4 = terms_4 + run_rake(sentence)\n",
    "\n",
    "fd_4 = get_freq_dist(terms_4)\n",
    "fd_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b93cc460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'’': 4, 'devil made': 3, 'series': 3, 'lorraine': 3, 'wan': 3, 'ed': 2, 'much': 2, 'two conjuring films': 1, 'several spinoffs established': 1, 'substantial following': 1, ...})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_5 = []\n",
    "for sentence in sentences_5:\n",
    "    terms_5 = terms_5 + run_rake(sentence)\n",
    "\n",
    "fd_5 = get_freq_dist(terms_5)\n",
    "fd_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c9b54fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'series': 5, 'devil made': 5, 'conjuring': 5, 'lorraine warren': 3, 'real': 3, 'warrens': 3, 'david glatzel': 3, 'johnson': 3, 'movie': 3, 'right': 2, ...})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_6 = []\n",
    "for sentence in sentences_6:\n",
    "    terms_6 = terms_6 + run_rake(sentence)\n",
    "\n",
    "fd_6 = get_freq_dist(terms_6)\n",
    "fd_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7684479d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'’': 11, 'devil made': 7, 'arne ’': 3, 'issue': 2, 'series': 2, 'one': 2, 'chaves': 2, 'innocence': 2, 'buy': 2, 'conjuring films': 1, ...})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_7 = []\n",
    "for sentence in sentences_7:\n",
    "    terms_7 = terms_7 + run_rake(sentence)\n",
    "\n",
    "fd_7 = get_freq_dist(terms_7)\n",
    "fd_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec2a2891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'’': 20, 'well': 6, '“': 4, 'film': 4, 'devil made': 3, 'conjuring': 3, 'though': 3, 'warrens': 3, 'work': 3, 'couple': 2, ...})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_8 = []\n",
    "for sentence in sentences_8:\n",
    "    terms_8 = terms_8 + run_rake(sentence)\n",
    "\n",
    "fd_8 = get_freq_dist(terms_8)\n",
    "fd_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b636785b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'’': 8, 'series': 2, 'see': 2, 'lorraine ’': 2, 'ed': 2, 'role': 2, 'even': 2, 'conjuring universe': 1, 'successful': 1, 'scary': 1, ...})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_9 = []\n",
    "for sentence in sentences_9:\n",
    "    terms_9 = terms_9 + run_rake(sentence)\n",
    "\n",
    "fd_9 = get_freq_dist(terms_9)\n",
    "fd_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9bb58f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'’': 24, 'devil made': 8, 'jump scares': 5, 'film': 5, 'conjuring': 4, 'audience': 4, 'story': 4, 'way': 3, 'la llorona': 3, 'ever': 3, ...})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_10 = []\n",
    "for sentence in sentences_10:\n",
    "    terms_10 = terms_10 + run_rake(sentence)\n",
    "\n",
    "fd_10 = get_freq_dist(terms_10)\n",
    "fd_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a3a07362",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_1_list = list(fd_1.most_common(10))\n",
    "fd_2_list = list(fd_2.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2e8f17eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FDDoc1</th>\n",
       "      <th>FDDoc1_Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conjuring</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>devil made</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watch</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>franchise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>say</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>movies</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>course</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>less</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FDDoc1  FDDoc1_Counts\n",
       "0   conjuring             11\n",
       "1  devil made             10\n",
       "2       movie              8\n",
       "3       watch              4\n",
       "4   franchise              3\n",
       "5         say              3\n",
       "6      movies              3\n",
       "7         sit              2\n",
       "8      course              2\n",
       "9        less              2"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_1_df = pd.DataFrame(fd_1_list, columns = ['FDDoc1', 'FDDoc1_Counts'])\n",
    "fd_1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3329fda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FDDoc2</th>\n",
       "      <th>FDDoc2_Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>devil made</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conjuring</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>franchise</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>focus</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sense</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>feature</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conjuring franchise</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enough</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                FDDoc2  FDDoc2_Counts\n",
       "0                 film             10\n",
       "1           devil made              6\n",
       "2            conjuring              6\n",
       "3            franchise              4\n",
       "4                  one              3\n",
       "5                focus              3\n",
       "6                sense              3\n",
       "7              feature              3\n",
       "8  conjuring franchise              2\n",
       "9               enough              2"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_2_df = pd.DataFrame(fd_2_list, columns = ['FDDoc2', 'FDDoc2_Counts'])\n",
    "fd_2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c9020d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this first to get sentences from text.\n",
    "sentences=run_nltk_sent_tokenizer(document1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a664b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/erinmcmahon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "475ae080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I must admit that when I sat down to watch the 2021 addition to \"The Conjuring\" franchise, I was not harboring much of any overly great expectations or hopes, because since the first movie it has been a steady downward slope.\n",
      "===================NLTK Tokenizer===================\n",
      "['I', 'must', 'admit', 'that', 'when', 'I', 'sat', 'down', 'to', 'watch', 'the', '2021', 'addition', 'to', '``', 'The', 'Conjuring', \"''\", 'franchise', ',', 'I', 'was', 'not', 'harboring', 'much', 'of', 'any', 'overly', 'great', 'expectations', 'or', 'hopes', ',', 'because', 'since', 'the', 'first', 'movie', 'it', 'has', 'been', 'a', 'steady', 'downward', 'slope', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['I must', 'must admit', 'admit that', 'that when', 'when I', 'I sat', 'sat down', 'down to', 'to watch', 'watch the', 'the 2021', '2021 addition', 'addition to', 'to ``', '`` The', 'The Conjuring', \"Conjuring ''\", \"'' franchise\", 'franchise ,', ', I', 'I was', 'was not', 'not harboring', 'harboring much', 'much of', 'of any', 'any overly', 'overly great', 'great expectations', 'expectations or', 'or hopes', 'hopes ,', ', because', 'because since', 'since the', 'the first', 'first movie', 'movie it', 'it has', 'has been', 'been a', 'a steady', 'steady downward', 'downward slope', 'slope .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['I must admit', 'must admit that', 'admit that when', 'that when I', 'when I sat', 'I sat down', 'sat down to', 'down to watch', 'to watch the', 'watch the 2021', 'the 2021 addition', '2021 addition to', 'addition to ``', 'to `` The', '`` The Conjuring', \"The Conjuring ''\", \"Conjuring '' franchise\", \"'' franchise ,\", 'franchise , I', ', I was', 'I was not', 'was not harboring', 'not harboring much', 'harboring much of', 'much of any', 'of any overly', 'any overly great', 'overly great expectations', 'great expectations or', 'expectations or hopes', 'or hopes ,', 'hopes , because', ', because since', 'because since the', 'since the first', 'the first movie', 'first movie it', 'movie it has', 'it has been', 'has been a', 'been a steady', 'a steady downward', 'steady downward slope', 'downward slope .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['steady downward slope', 'overly great expectations', 'must admit', 'harboring much', 'first movie', '2021 addition', 'watch', 'since', 'sat', 'hopes', 'franchise', 'conjuring']\n",
      "===================NLTK Tokenizer===================\n",
      "['I', 'must', 'admit', 'that', 'when', 'I', 'sat', 'down', 'to', 'watch', 'the', '2021', 'addition', 'to', '``', 'The', 'Conjuring', \"''\", 'franchise', ',', 'I', 'was', 'not', 'harboring', 'much', 'of', 'any', 'overly', 'great', 'expectations', 'or', 'hopes', ',', 'because', 'since', 'the', 'first', 'movie', 'it', 'has', 'been', 'a', 'steady', 'downward', 'slope', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['i', 'must', 'admit', 'that', 'when', 'i', 'sat', 'down', 'to', 'watch', 'the', '2021', 'addition', 'to', '``', 'the', 'conjuring', \"''\", 'franchise', ',', 'i', 'was', 'not', 'harboring', 'much', 'of', 'any', 'overly', 'great', 'expectations', 'or', 'hopes', ',', 'because', 'since', 'the', 'first', 'movie', 'it', 'has', 'been', 'a', 'steady', 'downward', 'slope', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['I', 'must', 'admit', 'I', 'sat', 'watch', '2021', 'addition', '``', 'The', 'Conjuring', \"''\", 'franchise', ',', 'I', 'harboring', 'much', 'overly', 'great', 'expectations', 'hopes', ',', 'since', 'first', 'movie', 'steady', 'downward', 'slope', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['I', 'must', 'admit', 'that', 'when', 'I', 'sat', 'down', 'to', 'watch', 'the', 'addition', 'to', 'The', 'Conjuring', 'franchise', 'I', 'was', 'not', 'harboring', 'much', 'of', 'any', 'overly', 'great', 'expectations', 'or', 'hopes', 'because', 'since', 'the', 'first', 'movie', 'it', 'has', 'been', 'a', 'steady', 'downward', 'slope']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['I', 'must', 'admit', 'that', 'when', 'I', 'sat', 'down', 'to', 'watch', 'the', '2021', 'addition', 'to', '``', 'The', 'Conjuring', \"''\", 'franchise', ',', 'I', 'was', 'not', 'harboring', 'much', 'of', 'any', 'overly', 'great', 'expectations', 'or', 'hopes', ',', 'because', 'since', 'the', 'first', 'movie', 'it', 'has', 'been', 'a', 'steady', 'downward', 'slope', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['I', 'must', 'admit', 'that', 'when', 'I', 'sat', 'down', 'to', 'watch', 'the', 'addition', 'to', 'The', 'Conjuring', 'franchise', 'I', 'was', 'not', 'harboring', 'much', 'of', 'any', 'overly', 'great', 'expectations', 'or', 'hopes', 'because', 'since', 'the', 'first', 'movie', 'it', 'has', 'been', 'a', 'steady', 'downward', 'slope']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['i', 'must', 'admit', 'that', 'when', 'i', 'sat', 'down', 'to', 'watch', 'the', '2021', 'addit', 'to', '``', 'the', 'conjur', '``', 'franchis', ',', 'i', 'wa', 'not', 'harbor', 'much', 'of', 'ani', 'overli', 'great', 'expect', 'or', 'hope', ',', 'becaus', 'sinc', 'the', 'first', 'movi', 'it', 'ha', 'been', 'a', 'steadi', 'downward', 'slope', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['I', 'must', 'admit', 'that', 'when', 'I', 'sat', 'down', 'to', 'watch', 'the', '2021', 'addition', 'to', '``', 'The', 'Conjuring', '``', 'franchise', ',', 'I', 'wa', 'not', 'harboring', 'much', 'of', 'any', 'overly', 'great', 'expectation', 'or', 'hope', ',', 'because', 'since', 'the', 'first', 'movie', 'it', 'ha', 'been', 'a', 'steady', 'downward', 'slope', '.']\n",
      "Still, as I had the chance to sit down and watch \"The Conjuring: The Devil Made Me Do It\" from writers David Leslie Johnson-McGoldrick and James Wan.\n",
      "===================NLTK Tokenizer===================\n",
      "['Still', ',', 'as', 'I', 'had', 'the', 'chance', 'to', 'sit', 'down', 'and', 'watch', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'from', 'writers', 'David', 'Leslie', 'Johnson-McGoldrick', 'and', 'James', 'Wan', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['Still ,', ', as', 'as I', 'I had', 'had the', 'the chance', 'chance to', 'to sit', 'sit down', 'down and', 'and watch', 'watch ``', '`` The', 'The Conjuring', 'Conjuring :', ': The', 'The Devil', 'Devil Made', 'Made Me', 'Me Do', 'Do It', \"It ''\", \"'' from\", 'from writers', 'writers David', 'David Leslie', 'Leslie Johnson-McGoldrick', 'Johnson-McGoldrick and', 'and James', 'James Wan', 'Wan .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['Still , as', ', as I', 'as I had', 'I had the', 'had the chance', 'the chance to', 'chance to sit', 'to sit down', 'sit down and', 'down and watch', 'and watch ``', 'watch `` The', '`` The Conjuring', 'The Conjuring :', 'Conjuring : The', ': The Devil', 'The Devil Made', 'Devil Made Me', 'Made Me Do', 'Me Do It', \"Do It ''\", \"It '' from\", \"'' from writers\", 'from writers David', 'writers David Leslie', 'David Leslie Johnson-McGoldrick', 'Leslie Johnson-McGoldrick and', 'Johnson-McGoldrick and James', 'and James Wan', 'James Wan .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['writers david leslie johnson', 'james wan', 'devil made', 'watch', 'still', 'sit', 'mcgoldrick', 'conjuring', 'chance']\n",
      "===================NLTK Tokenizer===================\n",
      "['Still', ',', 'as', 'I', 'had', 'the', 'chance', 'to', 'sit', 'down', 'and', 'watch', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'from', 'writers', 'David', 'Leslie', 'Johnson-McGoldrick', 'and', 'James', 'Wan', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['still', ',', 'as', 'i', 'had', 'the', 'chance', 'to', 'sit', 'down', 'and', 'watch', '``', 'the', 'conjuring', ':', 'the', 'devil', 'made', 'me', 'do', 'it', \"''\", 'from', 'writers', 'david', 'leslie', 'johnson-mcgoldrick', 'and', 'james', 'wan', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['Still', ',', 'I', 'chance', 'sit', 'watch', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'writers', 'David', 'Leslie', 'Johnson-McGoldrick', 'James', 'Wan', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['Still', 'as', 'I', 'had', 'the', 'chance', 'to', 'sit', 'down', 'and', 'watch', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'from', 'writers', 'David', 'Leslie', 'Johnson', 'McGoldrick', 'and', 'James', 'Wan']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['Still', ',', 'as', 'I', 'had', 'the', 'chance', 'to', 'sit', 'down', 'and', 'watch', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'from', 'writers', 'David', 'Leslie', 'Johnson-McGoldrick', 'and', 'James', 'Wan', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['Still', 'as', 'I', 'had', 'the', 'chance', 'to', 'sit', 'down', 'and', 'watch', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'from', 'writers', 'David', 'Leslie', 'Johnson', 'McGoldrick', 'and', 'James', 'Wan']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['still', ',', 'as', 'i', 'had', 'the', 'chanc', 'to', 'sit', 'down', 'and', 'watch', '``', 'the', 'conjur', ':', 'the', 'devil', 'made', 'me', 'do', 'it', '``', 'from', 'writer', 'david', 'lesli', 'johnson-mcgoldrick', 'and', 'jame', 'wan', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['Still', ',', 'a', 'I', 'had', 'the', 'chance', 'to', 'sit', 'down', 'and', 'watch', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', '``', 'from', 'writer', 'David', 'Leslie', 'Johnson-McGoldrick', 'and', 'James', 'Wan', '.']\n",
      "So of course I did it.\n",
      "===================NLTK Tokenizer===================\n",
      "['So', 'of', 'course', 'I', 'did', 'it', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['So of', 'of course', 'course I', 'I did', 'did it', 'it .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['So of course', 'of course I', 'course I did', 'I did it', 'did it .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['course']\n",
      "===================NLTK Tokenizer===================\n",
      "['So', 'of', 'course', 'I', 'did', 'it', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['so', 'of', 'course', 'i', 'did', 'it', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['So', 'course', 'I', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['So', 'of', 'course', 'I', 'did', 'it']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['So', 'of', 'course', 'I', 'did', 'it', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['So', 'of', 'course', 'I', 'did', 'it']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['so', 'of', 'cours', 'i', 'did', 'it', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['So', 'of', 'course', 'I', 'did', 'it', '.']\n",
      "And I have to say that director Michael Chaves managed to deliver a movie that was only slightly entertaining.\n",
      "===================NLTK Tokenizer===================\n",
      "['And', 'I', 'have', 'to', 'say', 'that', 'director', 'Michael', 'Chaves', 'managed', 'to', 'deliver', 'a', 'movie', 'that', 'was', 'only', 'slightly', 'entertaining', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['And I', 'I have', 'have to', 'to say', 'say that', 'that director', 'director Michael', 'Michael Chaves', 'Chaves managed', 'managed to', 'to deliver', 'deliver a', 'a movie', 'movie that', 'that was', 'was only', 'only slightly', 'slightly entertaining', 'entertaining .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['And I have', 'I have to', 'have to say', 'to say that', 'say that director', 'that director Michael', 'director Michael Chaves', 'Michael Chaves managed', 'Chaves managed to', 'managed to deliver', 'to deliver a', 'deliver a movie', 'a movie that', 'movie that was', 'that was only', 'was only slightly', 'only slightly entertaining', 'slightly entertaining .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['director michael chaves managed', 'slightly entertaining', 'say', 'movie', 'deliver']\n",
      "===================NLTK Tokenizer===================\n",
      "['And', 'I', 'have', 'to', 'say', 'that', 'director', 'Michael', 'Chaves', 'managed', 'to', 'deliver', 'a', 'movie', 'that', 'was', 'only', 'slightly', 'entertaining', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['and', 'i', 'have', 'to', 'say', 'that', 'director', 'michael', 'chaves', 'managed', 'to', 'deliver', 'a', 'movie', 'that', 'was', 'only', 'slightly', 'entertaining', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['And', 'I', 'say', 'director', 'Michael', 'Chaves', 'managed', 'deliver', 'movie', 'slightly', 'entertaining', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['And', 'I', 'have', 'to', 'say', 'that', 'director', 'Michael', 'Chaves', 'managed', 'to', 'deliver', 'a', 'movie', 'that', 'was', 'only', 'slightly', 'entertaining']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['And', 'I', 'have', 'to', 'say', 'that', 'director', 'Michael', 'Chaves', 'managed', 'to', 'deliver', 'a', 'movie', 'that', 'was', 'only', 'slightly', 'entertaining', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['And', 'I', 'have', 'to', 'say', 'that', 'director', 'Michael', 'Chaves', 'managed', 'to', 'deliver', 'a', 'movie', 'that', 'was', 'only', 'slightly', 'entertaining']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['and', 'i', 'have', 'to', 'say', 'that', 'director', 'michael', 'chave', 'manag', 'to', 'deliv', 'a', 'movi', 'that', 'wa', 'onli', 'slightli', 'entertain', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['And', 'I', 'have', 'to', 'say', 'that', 'director', 'Michael', 'Chaves', 'managed', 'to', 'deliver', 'a', 'movie', 'that', 'wa', 'only', 'slightly', 'entertaining', '.']\n",
      "\"The Conjuring: The Devil Made Me Do It\" was a whole lot of nothing going on, and you can essentially just watch the beginning and the last 25 minutes of the movie and skip on everything in between.\n",
      "===================NLTK Tokenizer===================\n",
      "['``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'was', 'a', 'whole', 'lot', 'of', 'nothing', 'going', 'on', ',', 'and', 'you', 'can', 'essentially', 'just', 'watch', 'the', 'beginning', 'and', 'the', 'last', '25', 'minutes', 'of', 'the', 'movie', 'and', 'skip', 'on', 'everything', 'in', 'between', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['`` The', 'The Conjuring', 'Conjuring :', ': The', 'The Devil', 'Devil Made', 'Made Me', 'Me Do', 'Do It', \"It ''\", \"'' was\", 'was a', 'a whole', 'whole lot', 'lot of', 'of nothing', 'nothing going', 'going on', 'on ,', ', and', 'and you', 'you can', 'can essentially', 'essentially just', 'just watch', 'watch the', 'the beginning', 'beginning and', 'and the', 'the last', 'last 25', '25 minutes', 'minutes of', 'of the', 'the movie', 'movie and', 'and skip', 'skip on', 'on everything', 'everything in', 'in between', 'between .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['`` The Conjuring', 'The Conjuring :', 'Conjuring : The', ': The Devil', 'The Devil Made', 'Devil Made Me', 'Made Me Do', 'Me Do It', \"Do It ''\", \"It '' was\", \"'' was a\", 'was a whole', 'a whole lot', 'whole lot of', 'lot of nothing', 'of nothing going', 'nothing going on', 'going on ,', 'on , and', ', and you', 'and you can', 'you can essentially', 'can essentially just', 'essentially just watch', 'just watch the', 'watch the beginning', 'the beginning and', 'beginning and the', 'and the last', 'the last 25', 'last 25 minutes', '25 minutes of', 'minutes of the', 'of the movie', 'the movie and', 'movie and skip', 'and skip on', 'skip on everything', 'on everything in', 'everything in between', 'in between .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['last 25 minutes', 'whole lot', 'nothing going', 'devil made', 'watch', 'skip', 'movie', 'everything', 'essentially', 'conjuring', 'beginning']\n",
      "===================NLTK Tokenizer===================\n",
      "['``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'was', 'a', 'whole', 'lot', 'of', 'nothing', 'going', 'on', ',', 'and', 'you', 'can', 'essentially', 'just', 'watch', 'the', 'beginning', 'and', 'the', 'last', '25', 'minutes', 'of', 'the', 'movie', 'and', 'skip', 'on', 'everything', 'in', 'between', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['``', 'the', 'conjuring', ':', 'the', 'devil', 'made', 'me', 'do', 'it', \"''\", 'was', 'a', 'whole', 'lot', 'of', 'nothing', 'going', 'on', ',', 'and', 'you', 'can', 'essentially', 'just', 'watch', 'the', 'beginning', 'and', 'the', 'last', '25', 'minutes', 'of', 'the', 'movie', 'and', 'skip', 'on', 'everything', 'in', 'between', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'whole', 'lot', 'nothing', 'going', ',', 'essentially', 'watch', 'beginning', 'last', '25', 'minutes', 'movie', 'skip', 'everything', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'was', 'a', 'whole', 'lot', 'of', 'nothing', 'going', 'on', 'and', 'you', 'can', 'essentially', 'just', 'watch', 'the', 'beginning', 'and', 'the', 'last', 'minutes', 'of', 'the', 'movie', 'and', 'skip', 'on', 'everything', 'in', 'between']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'was', 'a', 'whole', 'lot', 'of', 'nothing', 'going', 'on', ',', 'and', 'you', 'can', 'essentially', 'just', 'watch', 'the', 'beginning', 'and', 'the', 'last', '25', 'minutes', 'of', 'the', 'movie', 'and', 'skip', 'on', 'everything', 'in', 'between', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'was', 'a', 'whole', 'lot', 'of', 'nothing', 'going', 'on', 'and', 'you', 'can', 'essentially', 'just', 'watch', 'the', 'beginning', 'and', 'the', 'last', 'minutes', 'of', 'the', 'movie', 'and', 'skip', 'on', 'everything', 'in', 'between']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['``', 'the', 'conjur', ':', 'the', 'devil', 'made', 'me', 'do', 'it', '``', 'wa', 'a', 'whole', 'lot', 'of', 'noth', 'go', 'on', ',', 'and', 'you', 'can', 'essenti', 'just', 'watch', 'the', 'begin', 'and', 'the', 'last', '25', 'minut', 'of', 'the', 'movi', 'and', 'skip', 'on', 'everyth', 'in', 'between', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', '``', 'wa', 'a', 'whole', 'lot', 'of', 'nothing', 'going', 'on', ',', 'and', 'you', 'can', 'essentially', 'just', 'watch', 'the', 'beginning', 'and', 'the', 'last', '25', 'minute', 'of', 'the', 'movie', 'and', 'skip', 'on', 'everything', 'in', 'between', '.']\n",
      "The storyline written for \"The Conjuring: The Devil Made Me Do It\" was bland and slow paced, with very little of much excitement or interest happening in between the start and the end of the movie.\n",
      "===================NLTK Tokenizer===================\n",
      "['The', 'storyline', 'written', 'for', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'was', 'bland', 'and', 'slow', 'paced', ',', 'with', 'very', 'little', 'of', 'much', 'excitement', 'or', 'interest', 'happening', 'in', 'between', 'the', 'start', 'and', 'the', 'end', 'of', 'the', 'movie', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['The storyline', 'storyline written', 'written for', 'for ``', '`` The', 'The Conjuring', 'Conjuring :', ': The', 'The Devil', 'Devil Made', 'Made Me', 'Me Do', 'Do It', \"It ''\", \"'' was\", 'was bland', 'bland and', 'and slow', 'slow paced', 'paced ,', ', with', 'with very', 'very little', 'little of', 'of much', 'much excitement', 'excitement or', 'or interest', 'interest happening', 'happening in', 'in between', 'between the', 'the start', 'start and', 'and the', 'the end', 'end of', 'of the', 'the movie', 'movie .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['The storyline written', 'storyline written for', 'written for ``', 'for `` The', '`` The Conjuring', 'The Conjuring :', 'Conjuring : The', ': The Devil', 'The Devil Made', 'Devil Made Me', 'Made Me Do', 'Me Do It', \"Do It ''\", \"It '' was\", \"'' was bland\", 'was bland and', 'bland and slow', 'and slow paced', 'slow paced ,', 'paced , with', ', with very', 'with very little', 'very little of', 'little of much', 'of much excitement', 'much excitement or', 'excitement or interest', 'or interest happening', 'interest happening in', 'happening in between', 'in between the', 'between the start', 'the start and', 'start and the', 'and the end', 'the end of', 'end of the', 'of the movie', 'the movie .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['storyline written', 'slow paced', 'much excitement', 'interest happening', 'devil made', 'start', 'movie', 'little', 'end', 'conjuring', 'bland']\n",
      "===================NLTK Tokenizer===================\n",
      "['The', 'storyline', 'written', 'for', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'was', 'bland', 'and', 'slow', 'paced', ',', 'with', 'very', 'little', 'of', 'much', 'excitement', 'or', 'interest', 'happening', 'in', 'between', 'the', 'start', 'and', 'the', 'end', 'of', 'the', 'movie', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['the', 'storyline', 'written', 'for', '``', 'the', 'conjuring', ':', 'the', 'devil', 'made', 'me', 'do', 'it', \"''\", 'was', 'bland', 'and', 'slow', 'paced', ',', 'with', 'very', 'little', 'of', 'much', 'excitement', 'or', 'interest', 'happening', 'in', 'between', 'the', 'start', 'and', 'the', 'end', 'of', 'the', 'movie', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['The', 'storyline', 'written', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'bland', 'slow', 'paced', ',', 'little', 'much', 'excitement', 'interest', 'happening', 'start', 'end', 'movie', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['The', 'storyline', 'written', 'for', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'was', 'bland', 'and', 'slow', 'paced', 'with', 'very', 'little', 'of', 'much', 'excitement', 'or', 'interest', 'happening', 'in', 'between', 'the', 'start', 'and', 'the', 'end', 'of', 'the', 'movie']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['The', 'storyline', 'written', 'for', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'was', 'bland', 'and', 'slow', 'paced', ',', 'with', 'very', 'little', 'of', 'much', 'excitement', 'or', 'interest', 'happening', 'in', 'between', 'the', 'start', 'and', 'the', 'end', 'of', 'the', 'movie', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['The', 'storyline', 'written', 'for', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'was', 'bland', 'and', 'slow', 'paced', 'with', 'very', 'little', 'of', 'much', 'excitement', 'or', 'interest', 'happening', 'in', 'between', 'the', 'start', 'and', 'the', 'end', 'of', 'the', 'movie']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['the', 'storylin', 'written', 'for', '``', 'the', 'conjur', ':', 'the', 'devil', 'made', 'me', 'do', 'it', '``', 'wa', 'bland', 'and', 'slow', 'pace', ',', 'with', 'veri', 'littl', 'of', 'much', 'excit', 'or', 'interest', 'happen', 'in', 'between', 'the', 'start', 'and', 'the', 'end', 'of', 'the', 'movi', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['The', 'storyline', 'written', 'for', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', '``', 'wa', 'bland', 'and', 'slow', 'paced', ',', 'with', 'very', 'little', 'of', 'much', 'excitement', 'or', 'interest', 'happening', 'in', 'between', 'the', 'start', 'and', 'the', 'end', 'of', 'the', 'movie', '.']\n",
      "And that ultimately led to a less than mediocre movie experience for me.\n",
      "===================NLTK Tokenizer===================\n",
      "['And', 'that', 'ultimately', 'led', 'to', 'a', 'less', 'than', 'mediocre', 'movie', 'experience', 'for', 'me', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['And that', 'that ultimately', 'ultimately led', 'led to', 'to a', 'a less', 'less than', 'than mediocre', 'mediocre movie', 'movie experience', 'experience for', 'for me', 'me .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['And that ultimately', 'that ultimately led', 'ultimately led to', 'led to a', 'to a less', 'a less than', 'less than mediocre', 'than mediocre movie', 'mediocre movie experience', 'movie experience for', 'experience for me', 'for me .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['mediocre movie experience', 'ultimately led', 'less']\n",
      "===================NLTK Tokenizer===================\n",
      "['And', 'that', 'ultimately', 'led', 'to', 'a', 'less', 'than', 'mediocre', 'movie', 'experience', 'for', 'me', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['and', 'that', 'ultimately', 'led', 'to', 'a', 'less', 'than', 'mediocre', 'movie', 'experience', 'for', 'me', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['And', 'ultimately', 'led', 'less', 'mediocre', 'movie', 'experience', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['And', 'that', 'ultimately', 'led', 'to', 'a', 'less', 'than', 'mediocre', 'movie', 'experience', 'for', 'me']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['And', 'that', 'ultimately', 'led', 'to', 'a', 'less', 'than', 'mediocre', 'movie', 'experience', 'for', 'me', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['And', 'that', 'ultimately', 'led', 'to', 'a', 'less', 'than', 'mediocre', 'movie', 'experience', 'for', 'me']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['and', 'that', 'ultim', 'led', 'to', 'a', 'less', 'than', 'mediocr', 'movi', 'experi', 'for', 'me', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['And', 'that', 'ultimately', 'led', 'to', 'a', 'le', 'than', 'mediocre', 'movie', 'experience', 'for', 'me', '.']\n",
      "And yeah, I am a horror veteran, so \"The Conjuring: The Devil Made Me Do It\" was a walk in park.\n",
      "===================NLTK Tokenizer===================\n",
      "['And', 'yeah', ',', 'I', 'am', 'a', 'horror', 'veteran', ',', 'so', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'was', 'a', 'walk', 'in', 'park', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['And yeah', 'yeah ,', ', I', 'I am', 'am a', 'a horror', 'horror veteran', 'veteran ,', ', so', 'so ``', '`` The', 'The Conjuring', 'Conjuring :', ': The', 'The Devil', 'Devil Made', 'Made Me', 'Me Do', 'Do It', \"It ''\", \"'' was\", 'was a', 'a walk', 'walk in', 'in park', 'park .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['And yeah ,', 'yeah , I', ', I am', 'I am a', 'am a horror', 'a horror veteran', 'horror veteran ,', 'veteran , so', ', so ``', 'so `` The', '`` The Conjuring', 'The Conjuring :', 'Conjuring : The', ': The Devil', 'The Devil Made', 'Devil Made Me', 'Made Me Do', 'Me Do It', \"Do It ''\", \"It '' was\", \"'' was a\", 'was a walk', 'a walk in', 'walk in park', 'in park .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['horror veteran', 'devil made', 'yeah', 'walk', 'park', 'conjuring']\n",
      "===================NLTK Tokenizer===================\n",
      "['And', 'yeah', ',', 'I', 'am', 'a', 'horror', 'veteran', ',', 'so', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'was', 'a', 'walk', 'in', 'park', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['and', 'yeah', ',', 'i', 'am', 'a', 'horror', 'veteran', ',', 'so', '``', 'the', 'conjuring', ':', 'the', 'devil', 'made', 'me', 'do', 'it', \"''\", 'was', 'a', 'walk', 'in', 'park', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['And', 'yeah', ',', 'I', 'horror', 'veteran', ',', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'walk', 'park', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['And', 'yeah', 'I', 'am', 'a', 'horror', 'veteran', 'so', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'was', 'a', 'walk', 'in', 'park']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['And', 'yeah', ',', 'I', 'am', 'a', 'horror', 'veteran', ',', 'so', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'was', 'a', 'walk', 'in', 'park', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['And', 'yeah', 'I', 'am', 'a', 'horror', 'veteran', 'so', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'was', 'a', 'walk', 'in', 'park']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['and', 'yeah', ',', 'i', 'am', 'a', 'horror', 'veteran', ',', 'so', '``', 'the', 'conjur', ':', 'the', 'devil', 'made', 'me', 'do', 'it', '``', 'wa', 'a', 'walk', 'in', 'park', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['And', 'yeah', ',', 'I', 'am', 'a', 'horror', 'veteran', ',', 'so', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', '``', 'wa', 'a', 'walk', 'in', 'park', '.']\n",
      "There were a few jump scare moments to be had here, but they could be seen coming a mile away.\n",
      "===================NLTK Tokenizer===================\n",
      "['There', 'were', 'a', 'few', 'jump', 'scare', 'moments', 'to', 'be', 'had', 'here', ',', 'but', 'they', 'could', 'be', 'seen', 'coming', 'a', 'mile', 'away', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['There were', 'were a', 'a few', 'few jump', 'jump scare', 'scare moments', 'moments to', 'to be', 'be had', 'had here', 'here ,', ', but', 'but they', 'they could', 'could be', 'be seen', 'seen coming', 'coming a', 'a mile', 'mile away', 'away .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['There were a', 'were a few', 'a few jump', 'few jump scare', 'jump scare moments', 'scare moments to', 'moments to be', 'to be had', 'be had here', 'had here ,', 'here , but', ', but they', 'but they could', 'they could be', 'could be seen', 'be seen coming', 'seen coming a', 'coming a mile', 'a mile away', 'mile away .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['jump scare moments', 'seen coming', 'mile away', 'could']\n",
      "===================NLTK Tokenizer===================\n",
      "['There', 'were', 'a', 'few', 'jump', 'scare', 'moments', 'to', 'be', 'had', 'here', ',', 'but', 'they', 'could', 'be', 'seen', 'coming', 'a', 'mile', 'away', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['there', 'were', 'a', 'few', 'jump', 'scare', 'moments', 'to', 'be', 'had', 'here', ',', 'but', 'they', 'could', 'be', 'seen', 'coming', 'a', 'mile', 'away', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['There', 'jump', 'scare', 'moments', ',', 'could', 'seen', 'coming', 'mile', 'away', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['There', 'were', 'a', 'few', 'jump', 'scare', 'moments', 'to', 'be', 'had', 'here', 'but', 'they', 'could', 'be', 'seen', 'coming', 'a', 'mile', 'away']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['There', 'were', 'a', 'few', 'jump', 'scare', 'moments', 'to', 'be', 'had', 'here', ',', 'but', 'they', 'could', 'be', 'seen', 'coming', 'a', 'mile', 'away', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['There', 'were', 'a', 'few', 'jump', 'scare', 'moments', 'to', 'be', 'had', 'here', 'but', 'they', 'could', 'be', 'seen', 'coming', 'a', 'mile', 'away']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['there', 'were', 'a', 'few', 'jump', 'scare', 'moment', 'to', 'be', 'had', 'here', ',', 'but', 'they', 'could', 'be', 'seen', 'come', 'a', 'mile', 'away', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['There', 'were', 'a', 'few', 'jump', 'scare', 'moment', 'to', 'be', 'had', 'here', ',', 'but', 'they', 'could', 'be', 'seen', 'coming', 'a', 'mile', 'away', '.']\n",
      "And the storyline wasn't particularly dark, brooding or scary.\n",
      "===================NLTK Tokenizer===================\n",
      "['And', 'the', 'storyline', 'was', \"n't\", 'particularly', 'dark', ',', 'brooding', 'or', 'scary', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['And the', 'the storyline', 'storyline was', \"was n't\", \"n't particularly\", 'particularly dark', 'dark ,', ', brooding', 'brooding or', 'or scary', 'scary .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['And the storyline', 'the storyline was', \"storyline was n't\", \"was n't particularly\", \"n't particularly dark\", 'particularly dark ,', 'dark , brooding', ', brooding or', 'brooding or scary', 'or scary .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['particularly dark', 'storyline', 'scary', 'brooding']\n",
      "===================NLTK Tokenizer===================\n",
      "['And', 'the', 'storyline', 'was', \"n't\", 'particularly', 'dark', ',', 'brooding', 'or', 'scary', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['and', 'the', 'storyline', 'was', \"n't\", 'particularly', 'dark', ',', 'brooding', 'or', 'scary', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['And', 'storyline', \"n't\", 'particularly', 'dark', ',', 'brooding', 'scary', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['And', 'the', 'storyline', 'wasn', 't', 'particularly', 'dark', 'brooding', 'or', 'scary']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['And', 'the', 'storyline', 'was', \"n't\", 'particularly', 'dark', ',', 'brooding', 'or', 'scary', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['And', 'the', 'storyline', 'wasn', 't', 'particularly', 'dark', 'brooding', 'or', 'scary']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['and', 'the', 'storylin', 'wa', \"n't\", 'particularli', 'dark', ',', 'brood', 'or', 'scari', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['And', 'the', 'storyline', 'wa', \"n't\", 'particularly', 'dark', ',', 'brooding', 'or', 'scary', '.']\n",
      "So this was not an impressive foray into the horror genre.\n",
      "===================NLTK Tokenizer===================\n",
      "['So', 'this', 'was', 'not', 'an', 'impressive', 'foray', 'into', 'the', 'horror', 'genre', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['So this', 'this was', 'was not', 'not an', 'an impressive', 'impressive foray', 'foray into', 'into the', 'the horror', 'horror genre', 'genre .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['So this was', 'this was not', 'was not an', 'not an impressive', 'an impressive foray', 'impressive foray into', 'foray into the', 'into the horror', 'the horror genre', 'horror genre .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['impressive foray', 'horror genre']\n",
      "===================NLTK Tokenizer===================\n",
      "['So', 'this', 'was', 'not', 'an', 'impressive', 'foray', 'into', 'the', 'horror', 'genre', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['so', 'this', 'was', 'not', 'an', 'impressive', 'foray', 'into', 'the', 'horror', 'genre', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['So', 'impressive', 'foray', 'horror', 'genre', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['So', 'this', 'was', 'not', 'an', 'impressive', 'foray', 'into', 'the', 'horror', 'genre']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['So', 'this', 'was', 'not', 'an', 'impressive', 'foray', 'into', 'the', 'horror', 'genre', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['So', 'this', 'was', 'not', 'an', 'impressive', 'foray', 'into', 'the', 'horror', 'genre']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['so', 'thi', 'wa', 'not', 'an', 'impress', 'foray', 'into', 'the', 'horror', 'genr', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['So', 'this', 'wa', 'not', 'an', 'impressive', 'foray', 'into', 'the', 'horror', 'genre', '.']\n",
      "I will say that the special effects in \"The Conjuring: The Devil Made Me Do It\" were good, and they definitely added something worthwhile to the movie.\n",
      "===================NLTK Tokenizer===================\n",
      "['I', 'will', 'say', 'that', 'the', 'special', 'effects', 'in', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'were', 'good', ',', 'and', 'they', 'definitely', 'added', 'something', 'worthwhile', 'to', 'the', 'movie', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['I will', 'will say', 'say that', 'that the', 'the special', 'special effects', 'effects in', 'in ``', '`` The', 'The Conjuring', 'Conjuring :', ': The', 'The Devil', 'Devil Made', 'Made Me', 'Me Do', 'Do It', \"It ''\", \"'' were\", 'were good', 'good ,', ', and', 'and they', 'they definitely', 'definitely added', 'added something', 'something worthwhile', 'worthwhile to', 'to the', 'the movie', 'movie .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['I will say', 'will say that', 'say that the', 'that the special', 'the special effects', 'special effects in', 'effects in ``', 'in `` The', '`` The Conjuring', 'The Conjuring :', 'Conjuring : The', ': The Devil', 'The Devil Made', 'Devil Made Me', 'Made Me Do', 'Me Do It', \"Do It ''\", \"It '' were\", \"'' were good\", 'were good ,', 'good , and', ', and they', 'and they definitely', 'they definitely added', 'definitely added something', 'added something worthwhile', 'something worthwhile to', 'worthwhile to the', 'to the movie', 'the movie .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['definitely added something worthwhile', 'special effects', 'devil made', 'say', 'movie', 'good', 'conjuring']\n",
      "===================NLTK Tokenizer===================\n",
      "['I', 'will', 'say', 'that', 'the', 'special', 'effects', 'in', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'were', 'good', ',', 'and', 'they', 'definitely', 'added', 'something', 'worthwhile', 'to', 'the', 'movie', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['i', 'will', 'say', 'that', 'the', 'special', 'effects', 'in', '``', 'the', 'conjuring', ':', 'the', 'devil', 'made', 'me', 'do', 'it', \"''\", 'were', 'good', ',', 'and', 'they', 'definitely', 'added', 'something', 'worthwhile', 'to', 'the', 'movie', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['I', 'say', 'special', 'effects', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'good', ',', 'definitely', 'added', 'something', 'worthwhile', 'movie', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['I', 'will', 'say', 'that', 'the', 'special', 'effects', 'in', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'were', 'good', 'and', 'they', 'definitely', 'added', 'something', 'worthwhile', 'to', 'the', 'movie']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['I', 'will', 'say', 'that', 'the', 'special', 'effects', 'in', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'were', 'good', ',', 'and', 'they', 'definitely', 'added', 'something', 'worthwhile', 'to', 'the', 'movie', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['I', 'will', 'say', 'that', 'the', 'special', 'effects', 'in', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'were', 'good', 'and', 'they', 'definitely', 'added', 'something', 'worthwhile', 'to', 'the', 'movie']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['i', 'will', 'say', 'that', 'the', 'special', 'effect', 'in', '``', 'the', 'conjur', ':', 'the', 'devil', 'made', 'me', 'do', 'it', '``', 'were', 'good', ',', 'and', 'they', 'definit', 'ad', 'someth', 'worthwhil', 'to', 'the', 'movi', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['I', 'will', 'say', 'that', 'the', 'special', 'effect', 'in', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', '``', 'were', 'good', ',', 'and', 'they', 'definitely', 'added', 'something', 'worthwhile', 'to', 'the', 'movie', '.']\n",
      "And \"The Conjuring: The Devil Made Me Do It\" is not a movie that is a make or break experience with its special effects, but they do add a good element to the overall experience of the movie.\n",
      "===================NLTK Tokenizer===================\n",
      "['And', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'is', 'not', 'a', 'movie', 'that', 'is', 'a', 'make', 'or', 'break', 'experience', 'with', 'its', 'special', 'effects', ',', 'but', 'they', 'do', 'add', 'a', 'good', 'element', 'to', 'the', 'overall', 'experience', 'of', 'the', 'movie', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['And ``', '`` The', 'The Conjuring', 'Conjuring :', ': The', 'The Devil', 'Devil Made', 'Made Me', 'Me Do', 'Do It', \"It ''\", \"'' is\", 'is not', 'not a', 'a movie', 'movie that', 'that is', 'is a', 'a make', 'make or', 'or break', 'break experience', 'experience with', 'with its', 'its special', 'special effects', 'effects ,', ', but', 'but they', 'they do', 'do add', 'add a', 'a good', 'good element', 'element to', 'to the', 'the overall', 'overall experience', 'experience of', 'of the', 'the movie', 'movie .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['And `` The', '`` The Conjuring', 'The Conjuring :', 'Conjuring : The', ': The Devil', 'The Devil Made', 'Devil Made Me', 'Made Me Do', 'Me Do It', \"Do It ''\", \"It '' is\", \"'' is not\", 'is not a', 'not a movie', 'a movie that', 'movie that is', 'that is a', 'is a make', 'a make or', 'make or break', 'or break experience', 'break experience with', 'experience with its', 'with its special', 'its special effects', 'special effects ,', 'effects , but', ', but they', 'but they do', 'they do add', 'do add a', 'add a good', 'a good element', 'good element to', 'element to the', 'to the overall', 'the overall experience', 'overall experience of', 'experience of the', 'of the movie', 'the movie .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['special effects', 'overall experience', 'good element', 'devil made', 'break experience', 'movie', 'movie', 'make', 'conjuring', 'add']\n",
      "===================NLTK Tokenizer===================\n",
      "['And', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'is', 'not', 'a', 'movie', 'that', 'is', 'a', 'make', 'or', 'break', 'experience', 'with', 'its', 'special', 'effects', ',', 'but', 'they', 'do', 'add', 'a', 'good', 'element', 'to', 'the', 'overall', 'experience', 'of', 'the', 'movie', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['and', '``', 'the', 'conjuring', ':', 'the', 'devil', 'made', 'me', 'do', 'it', \"''\", 'is', 'not', 'a', 'movie', 'that', 'is', 'a', 'make', 'or', 'break', 'experience', 'with', 'its', 'special', 'effects', ',', 'but', 'they', 'do', 'add', 'a', 'good', 'element', 'to', 'the', 'overall', 'experience', 'of', 'the', 'movie', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['And', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'movie', 'make', 'break', 'experience', 'special', 'effects', ',', 'add', 'good', 'element', 'overall', 'experience', 'movie', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['And', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'is', 'not', 'a', 'movie', 'that', 'is', 'a', 'make', 'or', 'break', 'experience', 'with', 'its', 'special', 'effects', 'but', 'they', 'do', 'add', 'a', 'good', 'element', 'to', 'the', 'overall', 'experience', 'of', 'the', 'movie']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['And', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'is', 'not', 'a', 'movie', 'that', 'is', 'a', 'make', 'or', 'break', 'experience', 'with', 'its', 'special', 'effects', ',', 'but', 'they', 'do', 'add', 'a', 'good', 'element', 'to', 'the', 'overall', 'experience', 'of', 'the', 'movie', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['And', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'is', 'not', 'a', 'movie', 'that', 'is', 'a', 'make', 'or', 'break', 'experience', 'with', 'its', 'special', 'effects', 'but', 'they', 'do', 'add', 'a', 'good', 'element', 'to', 'the', 'overall', 'experience', 'of', 'the', 'movie']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['and', '``', 'the', 'conjur', ':', 'the', 'devil', 'made', 'me', 'do', 'it', '``', 'is', 'not', 'a', 'movi', 'that', 'is', 'a', 'make', 'or', 'break', 'experi', 'with', 'it', 'special', 'effect', ',', 'but', 'they', 'do', 'add', 'a', 'good', 'element', 'to', 'the', 'overal', 'experi', 'of', 'the', 'movi', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['And', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', '``', 'is', 'not', 'a', 'movie', 'that', 'is', 'a', 'make', 'or', 'break', 'experience', 'with', 'it', 'special', 'effect', ',', 'but', 'they', 'do', 'add', 'a', 'good', 'element', 'to', 'the', 'overall', 'experience', 'of', 'the', 'movie', '.']\n",
      "The acting in the movie was good, but of course we are talking about actor Patrick Wilson and actress Vera Farmiga here, whom singlehandedly have been carrying the franchise with their performances.\n",
      "===================NLTK Tokenizer===================\n",
      "['The', 'acting', 'in', 'the', 'movie', 'was', 'good', ',', 'but', 'of', 'course', 'we', 'are', 'talking', 'about', 'actor', 'Patrick', 'Wilson', 'and', 'actress', 'Vera', 'Farmiga', 'here', ',', 'whom', 'singlehandedly', 'have', 'been', 'carrying', 'the', 'franchise', 'with', 'their', 'performances', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['The acting', 'acting in', 'in the', 'the movie', 'movie was', 'was good', 'good ,', ', but', 'but of', 'of course', 'course we', 'we are', 'are talking', 'talking about', 'about actor', 'actor Patrick', 'Patrick Wilson', 'Wilson and', 'and actress', 'actress Vera', 'Vera Farmiga', 'Farmiga here', 'here ,', ', whom', 'whom singlehandedly', 'singlehandedly have', 'have been', 'been carrying', 'carrying the', 'the franchise', 'franchise with', 'with their', 'their performances', 'performances .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['The acting in', 'acting in the', 'in the movie', 'the movie was', 'movie was good', 'was good ,', 'good , but', ', but of', 'but of course', 'of course we', 'course we are', 'we are talking', 'are talking about', 'talking about actor', 'about actor Patrick', 'actor Patrick Wilson', 'Patrick Wilson and', 'Wilson and actress', 'and actress Vera', 'actress Vera Farmiga', 'Vera Farmiga here', 'Farmiga here ,', 'here , whom', ', whom singlehandedly', 'whom singlehandedly have', 'singlehandedly have been', 'have been carrying', 'been carrying the', 'carrying the franchise', 'the franchise with', 'franchise with their', 'with their performances', 'their performances .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['actress vera farmiga', 'actor patrick wilson', 'talking', 'singlehandedly', 'performances', 'movie', 'good', 'franchise', 'course', 'carrying', 'acting']\n",
      "===================NLTK Tokenizer===================\n",
      "['The', 'acting', 'in', 'the', 'movie', 'was', 'good', ',', 'but', 'of', 'course', 'we', 'are', 'talking', 'about', 'actor', 'Patrick', 'Wilson', 'and', 'actress', 'Vera', 'Farmiga', 'here', ',', 'whom', 'singlehandedly', 'have', 'been', 'carrying', 'the', 'franchise', 'with', 'their', 'performances', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['the', 'acting', 'in', 'the', 'movie', 'was', 'good', ',', 'but', 'of', 'course', 'we', 'are', 'talking', 'about', 'actor', 'patrick', 'wilson', 'and', 'actress', 'vera', 'farmiga', 'here', ',', 'whom', 'singlehandedly', 'have', 'been', 'carrying', 'the', 'franchise', 'with', 'their', 'performances', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['The', 'acting', 'movie', 'good', ',', 'course', 'talking', 'actor', 'Patrick', 'Wilson', 'actress', 'Vera', 'Farmiga', ',', 'singlehandedly', 'carrying', 'franchise', 'performances', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['The', 'acting', 'in', 'the', 'movie', 'was', 'good', 'but', 'of', 'course', 'we', 'are', 'talking', 'about', 'actor', 'Patrick', 'Wilson', 'and', 'actress', 'Vera', 'Farmiga', 'here', 'whom', 'singlehandedly', 'have', 'been', 'carrying', 'the', 'franchise', 'with', 'their', 'performances']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['The', 'acting', 'in', 'the', 'movie', 'was', 'good', ',', 'but', 'of', 'course', 'we', 'are', 'talking', 'about', 'actor', 'Patrick', 'Wilson', 'and', 'actress', 'Vera', 'Farmiga', 'here', ',', 'whom', 'singlehandedly', 'have', 'been', 'carrying', 'the', 'franchise', 'with', 'their', 'performances', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['The', 'acting', 'in', 'the', 'movie', 'was', 'good', 'but', 'of', 'course', 'we', 'are', 'talking', 'about', 'actor', 'Patrick', 'Wilson', 'and', 'actress', 'Vera', 'Farmiga', 'here', 'whom', 'singlehandedly', 'have', 'been', 'carrying', 'the', 'franchise', 'with', 'their', 'performances']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['the', 'act', 'in', 'the', 'movi', 'wa', 'good', ',', 'but', 'of', 'cours', 'we', 'are', 'talk', 'about', 'actor', 'patrick', 'wilson', 'and', 'actress', 'vera', 'farmiga', 'here', ',', 'whom', 'singlehandedli', 'have', 'been', 'carri', 'the', 'franchis', 'with', 'their', 'perform', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['The', 'acting', 'in', 'the', 'movie', 'wa', 'good', ',', 'but', 'of', 'course', 'we', 'are', 'talking', 'about', 'actor', 'Patrick', 'Wilson', 'and', 'actress', 'Vera', 'Farmiga', 'here', ',', 'whom', 'singlehandedly', 'have', 'been', 'carrying', 'the', 'franchise', 'with', 'their', 'performance', '.']\n",
      "And true to the formula, they are the ones making it watchable again with the 2021 \"The Conjuring: The Devil Made Me Do It\".\n",
      "===================NLTK Tokenizer===================\n",
      "['And', 'true', 'to', 'the', 'formula', ',', 'they', 'are', 'the', 'ones', 'making', 'it', 'watchable', 'again', 'with', 'the', '2021', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['And true', 'true to', 'to the', 'the formula', 'formula ,', ', they', 'they are', 'are the', 'the ones', 'ones making', 'making it', 'it watchable', 'watchable again', 'again with', 'with the', 'the 2021', '2021 ``', '`` The', 'The Conjuring', 'Conjuring :', ': The', 'The Devil', 'Devil Made', 'Made Me', 'Me Do', 'Do It', \"It ''\", \"'' .\"]\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['And true to', 'true to the', 'to the formula', 'the formula ,', 'formula , they', ', they are', 'they are the', 'are the ones', 'the ones making', 'ones making it', 'making it watchable', 'it watchable again', 'watchable again with', 'again with the', 'with the 2021', 'the 2021 ``', '2021 `` The', '`` The Conjuring', 'The Conjuring :', 'Conjuring : The', ': The Devil', 'The Devil Made', 'Devil Made Me', 'Made Me Do', 'Me Do It', \"Do It ''\", \"It '' .\"]\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['ones making', 'devil made', 'watchable', 'true', 'formula', 'conjuring', '2021', '\".']\n",
      "===================NLTK Tokenizer===================\n",
      "['And', 'true', 'to', 'the', 'formula', ',', 'they', 'are', 'the', 'ones', 'making', 'it', 'watchable', 'again', 'with', 'the', '2021', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['and', 'true', 'to', 'the', 'formula', ',', 'they', 'are', 'the', 'ones', 'making', 'it', 'watchable', 'again', 'with', 'the', '2021', '``', 'the', 'conjuring', ':', 'the', 'devil', 'made', 'me', 'do', 'it', \"''\", '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['And', 'true', 'formula', ',', 'ones', 'making', 'watchable', '2021', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['And', 'true', 'to', 'the', 'formula', 'they', 'are', 'the', 'ones', 'making', 'it', 'watchable', 'again', 'with', 'the', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['And', 'true', 'to', 'the', 'formula', ',', 'they', 'are', 'the', 'ones', 'making', 'it', 'watchable', 'again', 'with', 'the', '2021', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['And', 'true', 'to', 'the', 'formula', 'they', 'are', 'the', 'ones', 'making', 'it', 'watchable', 'again', 'with', 'the', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['and', 'true', 'to', 'the', 'formula', ',', 'they', 'are', 'the', 'one', 'make', 'it', 'watchabl', 'again', 'with', 'the', '2021', '``', 'the', 'conjur', ':', 'the', 'devil', 'made', 'me', 'do', 'it', '``', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['And', 'true', 'to', 'the', 'formula', ',', 'they', 'are', 'the', 'one', 'making', 'it', 'watchable', 'again', 'with', 'the', '2021', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', '``', '.']\n",
      "It should also be noted that the casting of Eugenie Bondurant as the occultist was a spot on choice, because she was genuinely creepy with her performance.\n",
      "===================NLTK Tokenizer===================\n",
      "['It', 'should', 'also', 'be', 'noted', 'that', 'the', 'casting', 'of', 'Eugenie', 'Bondurant', 'as', 'the', 'occultist', 'was', 'a', 'spot', 'on', 'choice', ',', 'because', 'she', 'was', 'genuinely', 'creepy', 'with', 'her', 'performance', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['It should', 'should also', 'also be', 'be noted', 'noted that', 'that the', 'the casting', 'casting of', 'of Eugenie', 'Eugenie Bondurant', 'Bondurant as', 'as the', 'the occultist', 'occultist was', 'was a', 'a spot', 'spot on', 'on choice', 'choice ,', ', because', 'because she', 'she was', 'was genuinely', 'genuinely creepy', 'creepy with', 'with her', 'her performance', 'performance .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['It should also', 'should also be', 'also be noted', 'be noted that', 'noted that the', 'that the casting', 'the casting of', 'casting of Eugenie', 'of Eugenie Bondurant', 'Eugenie Bondurant as', 'Bondurant as the', 'as the occultist', 'the occultist was', 'occultist was a', 'was a spot', 'a spot on', 'spot on choice', 'on choice ,', 'choice , because', ', because she', 'because she was', 'she was genuinely', 'was genuinely creepy', 'genuinely creepy with', 'creepy with her', 'with her performance', 'her performance .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['genuinely creepy', 'eugenie bondurant', 'spot', 'performance', 'occultist', 'noted', 'choice', 'casting', 'also']\n",
      "===================NLTK Tokenizer===================\n",
      "['It', 'should', 'also', 'be', 'noted', 'that', 'the', 'casting', 'of', 'Eugenie', 'Bondurant', 'as', 'the', 'occultist', 'was', 'a', 'spot', 'on', 'choice', ',', 'because', 'she', 'was', 'genuinely', 'creepy', 'with', 'her', 'performance', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['it', 'should', 'also', 'be', 'noted', 'that', 'the', 'casting', 'of', 'eugenie', 'bondurant', 'as', 'the', 'occultist', 'was', 'a', 'spot', 'on', 'choice', ',', 'because', 'she', 'was', 'genuinely', 'creepy', 'with', 'her', 'performance', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['It', 'also', 'noted', 'casting', 'Eugenie', 'Bondurant', 'occultist', 'spot', 'choice', ',', 'genuinely', 'creepy', 'performance', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['It', 'should', 'also', 'be', 'noted', 'that', 'the', 'casting', 'of', 'Eugenie', 'Bondurant', 'as', 'the', 'occultist', 'was', 'a', 'spot', 'on', 'choice', 'because', 'she', 'was', 'genuinely', 'creepy', 'with', 'her', 'performance']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['It', 'should', 'also', 'be', 'noted', 'that', 'the', 'casting', 'of', 'Eugenie', 'Bondurant', 'as', 'the', 'occultist', 'was', 'a', 'spot', 'on', 'choice', ',', 'because', 'she', 'was', 'genuinely', 'creepy', 'with', 'her', 'performance', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['It', 'should', 'also', 'be', 'noted', 'that', 'the', 'casting', 'of', 'Eugenie', 'Bondurant', 'as', 'the', 'occultist', 'was', 'a', 'spot', 'on', 'choice', 'because', 'she', 'was', 'genuinely', 'creepy', 'with', 'her', 'performance']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['it', 'should', 'also', 'be', 'note', 'that', 'the', 'cast', 'of', 'eugeni', 'bondur', 'as', 'the', 'occultist', 'wa', 'a', 'spot', 'on', 'choic', ',', 'becaus', 'she', 'wa', 'genuin', 'creepi', 'with', 'her', 'perform', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['It', 'should', 'also', 'be', 'noted', 'that', 'the', 'casting', 'of', 'Eugenie', 'Bondurant', 'a', 'the', 'occultist', 'wa', 'a', 'spot', 'on', 'choice', ',', 'because', 'she', 'wa', 'genuinely', 'creepy', 'with', 'her', 'performance', '.']\n",
      "As I entered this movie with no expectations I can't say that I was disappointed with the end result.\n",
      "===================NLTK Tokenizer===================\n",
      "['As', 'I', 'entered', 'this', 'movie', 'with', 'no', 'expectations', 'I', 'ca', \"n't\", 'say', 'that', 'I', 'was', 'disappointed', 'with', 'the', 'end', 'result', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['As I', 'I entered', 'entered this', 'this movie', 'movie with', 'with no', 'no expectations', 'expectations I', 'I ca', \"ca n't\", \"n't say\", 'say that', 'that I', 'I was', 'was disappointed', 'disappointed with', 'with the', 'the end', 'end result', 'result .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['As I entered', 'I entered this', 'entered this movie', 'this movie with', 'movie with no', 'with no expectations', 'no expectations I', 'expectations I ca', \"I ca n't\", \"ca n't say\", \"n't say that\", 'say that I', 'that I was', 'I was disappointed', 'was disappointed with', 'disappointed with the', 'with the end', 'the end result', 'end result .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['end result', 'say', 'movie', 'expectations', 'entered', 'disappointed']\n",
      "===================NLTK Tokenizer===================\n",
      "['As', 'I', 'entered', 'this', 'movie', 'with', 'no', 'expectations', 'I', 'ca', \"n't\", 'say', 'that', 'I', 'was', 'disappointed', 'with', 'the', 'end', 'result', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['as', 'i', 'entered', 'this', 'movie', 'with', 'no', 'expectations', 'i', 'ca', \"n't\", 'say', 'that', 'i', 'was', 'disappointed', 'with', 'the', 'end', 'result', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['As', 'I', 'entered', 'movie', 'expectations', 'I', 'ca', \"n't\", 'say', 'I', 'disappointed', 'end', 'result', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['As', 'I', 'entered', 'this', 'movie', 'with', 'no', 'expectations', 'I', 'can', 't', 'say', 'that', 'I', 'was', 'disappointed', 'with', 'the', 'end', 'result']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['As', 'I', 'entered', 'this', 'movie', 'with', 'no', 'expectations', 'I', 'ca', \"n't\", 'say', 'that', 'I', 'was', 'disappointed', 'with', 'the', 'end', 'result', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['As', 'I', 'entered', 'this', 'movie', 'with', 'no', 'expectations', 'I', 'can', 't', 'say', 'that', 'I', 'was', 'disappointed', 'with', 'the', 'end', 'result']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['as', 'i', 'enter', 'thi', 'movi', 'with', 'no', 'expect', 'i', 'ca', \"n't\", 'say', 'that', 'i', 'wa', 'disappoint', 'with', 'the', 'end', 'result', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['As', 'I', 'entered', 'this', 'movie', 'with', 'no', 'expectation', 'I', 'ca', \"n't\", 'say', 'that', 'I', 'wa', 'disappointed', 'with', 'the', 'end', 'result', '.']\n",
      "But take heed, this is by no means a milestone in the horror cinema.\n",
      "===================NLTK Tokenizer===================\n",
      "['But', 'take', 'heed', ',', 'this', 'is', 'by', 'no', 'means', 'a', 'milestone', 'in', 'the', 'horror', 'cinema', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['But take', 'take heed', 'heed ,', ', this', 'this is', 'is by', 'by no', 'no means', 'means a', 'a milestone', 'milestone in', 'in the', 'the horror', 'horror cinema', 'cinema .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['But take heed', 'take heed ,', 'heed , this', ', this is', 'this is by', 'is by no', 'by no means', 'no means a', 'means a milestone', 'a milestone in', 'milestone in the', 'in the horror', 'the horror cinema', 'horror cinema .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['take heed', 'horror cinema', 'milestone', 'means']\n",
      "===================NLTK Tokenizer===================\n",
      "['But', 'take', 'heed', ',', 'this', 'is', 'by', 'no', 'means', 'a', 'milestone', 'in', 'the', 'horror', 'cinema', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['but', 'take', 'heed', ',', 'this', 'is', 'by', 'no', 'means', 'a', 'milestone', 'in', 'the', 'horror', 'cinema', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['But', 'take', 'heed', ',', 'means', 'milestone', 'horror', 'cinema', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['But', 'take', 'heed', 'this', 'is', 'by', 'no', 'means', 'a', 'milestone', 'in', 'the', 'horror', 'cinema']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['But', 'take', 'heed', ',', 'this', 'is', 'by', 'no', 'means', 'a', 'milestone', 'in', 'the', 'horror', 'cinema', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['But', 'take', 'heed', 'this', 'is', 'by', 'no', 'means', 'a', 'milestone', 'in', 'the', 'horror', 'cinema']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['but', 'take', 'heed', ',', 'thi', 'is', 'by', 'no', 'mean', 'a', 'mileston', 'in', 'the', 'horror', 'cinema', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['But', 'take', 'heed', ',', 'this', 'is', 'by', 'no', 'mean', 'a', 'milestone', 'in', 'the', 'horror', 'cinema', '.']\n",
      "\"The Conjuring: The Devil Made Me Do It\" came and went without leaving a lasting impression for me, just as the numerous other movies in the franchise - as their spin-off movies - have done.\n",
      "===================NLTK Tokenizer===================\n",
      "['``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'came', 'and', 'went', 'without', 'leaving', 'a', 'lasting', 'impression', 'for', 'me', ',', 'just', 'as', 'the', 'numerous', 'other', 'movies', 'in', 'the', 'franchise', '-', 'as', 'their', 'spin-off', 'movies', '-', 'have', 'done', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['`` The', 'The Conjuring', 'Conjuring :', ': The', 'The Devil', 'Devil Made', 'Made Me', 'Me Do', 'Do It', \"It ''\", \"'' came\", 'came and', 'and went', 'went without', 'without leaving', 'leaving a', 'a lasting', 'lasting impression', 'impression for', 'for me', 'me ,', ', just', 'just as', 'as the', 'the numerous', 'numerous other', 'other movies', 'movies in', 'in the', 'the franchise', 'franchise -', '- as', 'as their', 'their spin-off', 'spin-off movies', 'movies -', '- have', 'have done', 'done .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['`` The Conjuring', 'The Conjuring :', 'Conjuring : The', ': The Devil', 'The Devil Made', 'Devil Made Me', 'Made Me Do', 'Me Do It', \"Do It ''\", \"It '' came\", \"'' came and\", 'came and went', 'and went without', 'went without leaving', 'without leaving a', 'leaving a lasting', 'a lasting impression', 'lasting impression for', 'impression for me', 'for me ,', 'me , just', ', just as', 'just as the', 'as the numerous', 'the numerous other', 'numerous other movies', 'other movies in', 'movies in the', 'in the franchise', 'the franchise -', 'franchise - as', '- as their', 'as their spin-off', 'their spin-off movies', 'spin-off movies -', 'movies - have', '- have done', 'have done .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['went without leaving', 'lasting impression', 'devil made', 'spin', 'numerous', 'movies', 'movies', 'franchise', 'done', 'conjuring', 'came']\n",
      "===================NLTK Tokenizer===================\n",
      "['``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'came', 'and', 'went', 'without', 'leaving', 'a', 'lasting', 'impression', 'for', 'me', ',', 'just', 'as', 'the', 'numerous', 'other', 'movies', 'in', 'the', 'franchise', '-', 'as', 'their', 'spin-off', 'movies', '-', 'have', 'done', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['``', 'the', 'conjuring', ':', 'the', 'devil', 'made', 'me', 'do', 'it', \"''\", 'came', 'and', 'went', 'without', 'leaving', 'a', 'lasting', 'impression', 'for', 'me', ',', 'just', 'as', 'the', 'numerous', 'other', 'movies', 'in', 'the', 'franchise', '-', 'as', 'their', 'spin-off', 'movies', '-', 'have', 'done', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'came', 'went', 'without', 'leaving', 'lasting', 'impression', ',', 'numerous', 'movies', 'franchise', '-', 'spin-off', 'movies', '-', 'done', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'came', 'and', 'went', 'without', 'leaving', 'a', 'lasting', 'impression', 'for', 'me', 'just', 'as', 'the', 'numerous', 'other', 'movies', 'in', 'the', 'franchise', 'as', 'their', 'spin', 'off', 'movies', 'have', 'done']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'came', 'and', 'went', 'without', 'leaving', 'a', 'lasting', 'impression', 'for', 'me', ',', 'just', 'as', 'the', 'numerous', 'other', 'movies', 'in', 'the', 'franchise', '-', 'as', 'their', 'spin-off', 'movies', '-', 'have', 'done', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'came', 'and', 'went', 'without', 'leaving', 'a', 'lasting', 'impression', 'for', 'me', 'just', 'as', 'the', 'numerous', 'other', 'movies', 'in', 'the', 'franchise', 'as', 'their', 'spin', 'off', 'movies', 'have', 'done']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['``', 'the', 'conjur', ':', 'the', 'devil', 'made', 'me', 'do', 'it', '``', 'came', 'and', 'went', 'without', 'leav', 'a', 'last', 'impress', 'for', 'me', ',', 'just', 'as', 'the', 'numer', 'other', 'movi', 'in', 'the', 'franchis', '-', 'as', 'their', 'spin-off', 'movi', '-', 'have', 'done', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', '``', 'came', 'and', 'went', 'without', 'leaving', 'a', 'lasting', 'impression', 'for', 'me', ',', 'just', 'a', 'the', 'numerous', 'other', 'movie', 'in', 'the', 'franchise', '-', 'a', 'their', 'spin-off', 'movie', '-', 'have', 'done', '.']\n",
      "I can't tell them apart if you were to ask me what the second movie was about, for instance.\n",
      "===================NLTK Tokenizer===================\n",
      "['I', 'ca', \"n't\", 'tell', 'them', 'apart', 'if', 'you', 'were', 'to', 'ask', 'me', 'what', 'the', 'second', 'movie', 'was', 'about', ',', 'for', 'instance', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['I ca', \"ca n't\", \"n't tell\", 'tell them', 'them apart', 'apart if', 'if you', 'you were', 'were to', 'to ask', 'ask me', 'me what', 'what the', 'the second', 'second movie', 'movie was', 'was about', 'about ,', ', for', 'for instance', 'instance .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "[\"I ca n't\", \"ca n't tell\", \"n't tell them\", 'tell them apart', 'them apart if', 'apart if you', 'if you were', 'you were to', 'were to ask', 'to ask me', 'ask me what', 'me what the', 'what the second', 'the second movie', 'second movie was', 'movie was about', 'was about ,', 'about , for', ', for instance', 'for instance .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['second movie', 'tell', 'instance', 'ask', 'apart']\n",
      "===================NLTK Tokenizer===================\n",
      "['I', 'ca', \"n't\", 'tell', 'them', 'apart', 'if', 'you', 'were', 'to', 'ask', 'me', 'what', 'the', 'second', 'movie', 'was', 'about', ',', 'for', 'instance', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['i', 'ca', \"n't\", 'tell', 'them', 'apart', 'if', 'you', 'were', 'to', 'ask', 'me', 'what', 'the', 'second', 'movie', 'was', 'about', ',', 'for', 'instance', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['I', 'ca', \"n't\", 'tell', 'apart', 'ask', 'second', 'movie', ',', 'instance', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['I', 'can', 't', 'tell', 'them', 'apart', 'if', 'you', 'were', 'to', 'ask', 'me', 'what', 'the', 'second', 'movie', 'was', 'about', 'for', 'instance']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['I', 'ca', \"n't\", 'tell', 'them', 'apart', 'if', 'you', 'were', 'to', 'ask', 'me', 'what', 'the', 'second', 'movie', 'was', 'about', ',', 'for', 'instance', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['I', 'can', 't', 'tell', 'them', 'apart', 'if', 'you', 'were', 'to', 'ask', 'me', 'what', 'the', 'second', 'movie', 'was', 'about', 'for', 'instance']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['i', 'ca', \"n't\", 'tell', 'them', 'apart', 'if', 'you', 'were', 'to', 'ask', 'me', 'what', 'the', 'second', 'movi', 'wa', 'about', ',', 'for', 'instanc', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['I', 'ca', \"n't\", 'tell', 'them', 'apart', 'if', 'you', 'were', 'to', 'ask', 'me', 'what', 'the', 'second', 'movie', 'wa', 'about', ',', 'for', 'instance', '.']\n",
      "My rating of the 2021 \"The Conjuring: The Devil Made Me Do It\" lands on a less than mediocre four out of ten stars.\n",
      "===================NLTK Tokenizer===================\n",
      "['My', 'rating', 'of', 'the', '2021', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'lands', 'on', 'a', 'less', 'than', 'mediocre', 'four', 'out', 'of', 'ten', 'stars', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['My rating', 'rating of', 'of the', 'the 2021', '2021 ``', '`` The', 'The Conjuring', 'Conjuring :', ': The', 'The Devil', 'Devil Made', 'Made Me', 'Me Do', 'Do It', \"It ''\", \"'' lands\", 'lands on', 'on a', 'a less', 'less than', 'than mediocre', 'mediocre four', 'four out', 'out of', 'of ten', 'ten stars', 'stars .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['My rating of', 'rating of the', 'of the 2021', 'the 2021 ``', '2021 `` The', '`` The Conjuring', 'The Conjuring :', 'Conjuring : The', ': The Devil', 'The Devil Made', 'Devil Made Me', 'Made Me Do', 'Me Do It', \"Do It ''\", \"It '' lands\", \"'' lands on\", 'lands on a', 'on a less', 'a less than', 'less than mediocre', 'than mediocre four', 'mediocre four out', 'four out of', 'out of ten', 'of ten stars', 'ten stars .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['ten stars', 'mediocre four', 'devil made', 'rating', 'less', 'lands', 'conjuring', '2021']\n",
      "===================NLTK Tokenizer===================\n",
      "['My', 'rating', 'of', 'the', '2021', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'lands', 'on', 'a', 'less', 'than', 'mediocre', 'four', 'out', 'of', 'ten', 'stars', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['my', 'rating', 'of', 'the', '2021', '``', 'the', 'conjuring', ':', 'the', 'devil', 'made', 'me', 'do', 'it', \"''\", 'lands', 'on', 'a', 'less', 'than', 'mediocre', 'four', 'out', 'of', 'ten', 'stars', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['My', 'rating', '2021', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'lands', 'less', 'mediocre', 'four', 'ten', 'stars', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['My', 'rating', 'of', 'the', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'lands', 'on', 'a', 'less', 'than', 'mediocre', 'four', 'out', 'of', 'ten', 'stars']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['My', 'rating', 'of', 'the', '2021', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'lands', 'on', 'a', 'less', 'than', 'mediocre', 'four', 'out', 'of', 'ten', 'stars', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['My', 'rating', 'of', 'the', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'lands', 'on', 'a', 'less', 'than', 'mediocre', 'four', 'out', 'of', 'ten', 'stars']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['my', 'rate', 'of', 'the', '2021', '``', 'the', 'conjur', ':', 'the', 'devil', 'made', 'me', 'do', 'it', '``', 'land', 'on', 'a', 'less', 'than', 'mediocr', 'four', 'out', 'of', 'ten', 'star', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['My', 'rating', 'of', 'the', '2021', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', '``', 'land', 'on', 'a', 'le', 'than', 'mediocre', 'four', 'out', 'of', 'ten', 'star', '.']\n",
      "Sure, it was a watchable movie, and if you are new to the horror genre, then you will be in for something good I suppose.\n",
      "===================NLTK Tokenizer===================\n",
      "['Sure', ',', 'it', 'was', 'a', 'watchable', 'movie', ',', 'and', 'if', 'you', 'are', 'new', 'to', 'the', 'horror', 'genre', ',', 'then', 'you', 'will', 'be', 'in', 'for', 'something', 'good', 'I', 'suppose', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['Sure ,', ', it', 'it was', 'was a', 'a watchable', 'watchable movie', 'movie ,', ', and', 'and if', 'if you', 'you are', 'are new', 'new to', 'to the', 'the horror', 'horror genre', 'genre ,', ', then', 'then you', 'you will', 'will be', 'be in', 'in for', 'for something', 'something good', 'good I', 'I suppose', 'suppose .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['Sure , it', ', it was', 'it was a', 'was a watchable', 'a watchable movie', 'watchable movie ,', 'movie , and', ', and if', 'and if you', 'if you are', 'you are new', 'are new to', 'new to the', 'to the horror', 'the horror genre', 'horror genre ,', 'genre , then', ', then you', 'then you will', 'you will be', 'will be in', 'be in for', 'in for something', 'for something good', 'something good I', 'good I suppose', 'I suppose .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['watchable movie', 'something good', 'horror genre', 'sure', 'suppose', 'new']\n",
      "===================NLTK Tokenizer===================\n",
      "['Sure', ',', 'it', 'was', 'a', 'watchable', 'movie', ',', 'and', 'if', 'you', 'are', 'new', 'to', 'the', 'horror', 'genre', ',', 'then', 'you', 'will', 'be', 'in', 'for', 'something', 'good', 'I', 'suppose', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['sure', ',', 'it', 'was', 'a', 'watchable', 'movie', ',', 'and', 'if', 'you', 'are', 'new', 'to', 'the', 'horror', 'genre', ',', 'then', 'you', 'will', 'be', 'in', 'for', 'something', 'good', 'i', 'suppose', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['Sure', ',', 'watchable', 'movie', ',', 'new', 'horror', 'genre', ',', 'something', 'good', 'I', 'suppose', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['Sure', 'it', 'was', 'a', 'watchable', 'movie', 'and', 'if', 'you', 'are', 'new', 'to', 'the', 'horror', 'genre', 'then', 'you', 'will', 'be', 'in', 'for', 'something', 'good', 'I', 'suppose']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['Sure', ',', 'it', 'was', 'a', 'watchable', 'movie', ',', 'and', 'if', 'you', 'are', 'new', 'to', 'the', 'horror', 'genre', ',', 'then', 'you', 'will', 'be', 'in', 'for', 'something', 'good', 'I', 'suppose', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['Sure', 'it', 'was', 'a', 'watchable', 'movie', 'and', 'if', 'you', 'are', 'new', 'to', 'the', 'horror', 'genre', 'then', 'you', 'will', 'be', 'in', 'for', 'something', 'good', 'I', 'suppose']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['sure', ',', 'it', 'wa', 'a', 'watchabl', 'movi', ',', 'and', 'if', 'you', 'are', 'new', 'to', 'the', 'horror', 'genr', ',', 'then', 'you', 'will', 'be', 'in', 'for', 'someth', 'good', 'i', 'suppos', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['Sure', ',', 'it', 'wa', 'a', 'watchable', 'movie', ',', 'and', 'if', 'you', 'are', 'new', 'to', 'the', 'horror', 'genre', ',', 'then', 'you', 'will', 'be', 'in', 'for', 'something', 'good', 'I', 'suppose', '.']\n",
      "But if you expect a bit more from the movies you sit down to watch then \"The Conjuring: The Devil Made Me Do It\" just doesn't cut it.\n",
      "===================NLTK Tokenizer===================\n",
      "['But', 'if', 'you', 'expect', 'a', 'bit', 'more', 'from', 'the', 'movies', 'you', 'sit', 'down', 'to', 'watch', 'then', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'just', 'does', \"n't\", 'cut', 'it', '.']\n",
      "===================NLTK Word NGRAM Tokenizer 2 words===================\n",
      "['But if', 'if you', 'you expect', 'expect a', 'a bit', 'bit more', 'more from', 'from the', 'the movies', 'movies you', 'you sit', 'sit down', 'down to', 'to watch', 'watch then', 'then ``', '`` The', 'The Conjuring', 'Conjuring :', ': The', 'The Devil', 'Devil Made', 'Made Me', 'Me Do', 'Do It', \"It ''\", \"'' just\", 'just does', \"does n't\", \"n't cut\", 'cut it', 'it .']\n",
      "===================NLTK Word NGRAM Tokenizer 3 words===================\n",
      "['But if you', 'if you expect', 'you expect a', 'expect a bit', 'a bit more', 'bit more from', 'more from the', 'from the movies', 'the movies you', 'movies you sit', 'you sit down', 'sit down to', 'down to watch', 'to watch then', 'watch then ``', 'then `` The', '`` The Conjuring', 'The Conjuring :', 'Conjuring : The', ': The Devil', 'The Devil Made', 'Devil Made Me', 'Made Me Do', 'Me Do It', \"Do It ''\", \"It '' just\", \"'' just does\", \"just does n't\", \"does n't cut\", \"n't cut it\", 'cut it .']\n",
      "===================Phrase Machine===================\n",
      "===================Rake===================\n",
      "['devil made', 'watch', 'sit', 'movies', 'expect', 'cut', 'conjuring', 'bit']\n",
      "===================NLTK Tokenizer===================\n",
      "['But', 'if', 'you', 'expect', 'a', 'bit', 'more', 'from', 'the', 'movies', 'you', 'sit', 'down', 'to', 'watch', 'then', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'just', 'does', \"n't\", 'cut', 'it', '.']\n",
      "===================NLTK Tokenizer LOWER CASE===================\n",
      "['but', 'if', 'you', 'expect', 'a', 'bit', 'more', 'from', 'the', 'movies', 'you', 'sit', 'down', 'to', 'watch', 'then', '``', 'the', 'conjuring', ':', 'the', 'devil', 'made', 'me', 'do', 'it', \"''\", 'just', 'does', \"n't\", 'cut', 'it', '.']\n",
      "===================NLTK Tokenizer REMOVE STOP WORDS===================\n",
      "['But', 'expect', 'bit', 'movies', 'sit', 'watch', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", \"n't\", 'cut', '.']\n",
      "===================NLTK Tokenizer REMOVED PUNCTUATION===================\n",
      "['But', 'if', 'you', 'expect', 'a', 'bit', 'more', 'from', 'the', 'movies', 'you', 'sit', 'down', 'to', 'watch', 'then', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'just', 'doesn', 't', 'cut', 'it']\n",
      "===================NLTK Tokenizer REMOVED TAGS===================\n",
      "['But', 'if', 'you', 'expect', 'a', 'bit', 'more', 'from', 'the', 'movies', 'you', 'sit', 'down', 'to', 'watch', 'then', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', \"''\", 'just', 'does', \"n't\", 'cut', 'it', '.']\n",
      "===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\n",
      "['But', 'if', 'you', 'expect', 'a', 'bit', 'more', 'from', 'the', 'movies', 'you', 'sit', 'down', 'to', 'watch', 'then', 'The', 'Conjuring', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', 'just', 'doesn', 't', 'cut', 'it']\n",
      "===================NLTK Tokenizer STEMMING APPLIED===================\n",
      "['but', 'if', 'you', 'expect', 'a', 'bit', 'more', 'from', 'the', 'movi', 'you', 'sit', 'down', 'to', 'watch', 'then', '``', 'the', 'conjur', ':', 'the', 'devil', 'made', 'me', 'do', 'it', '``', 'just', 'doe', \"n't\", 'cut', 'it', '.']\n",
      "===================NLTK Tokenizer LEMMATIZATION APPLIED===================\n",
      "['But', 'if', 'you', 'expect', 'a', 'bit', 'more', 'from', 'the', 'movie', 'you', 'sit', 'down', 'to', 'watch', 'then', '``', 'The', 'Conjuring', ':', 'The', 'Devil', 'Made', 'Me', 'Do', 'It', '``', 'just', 'doe', \"n't\", 'cut', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "#Explore different extractors and difference preprocessing techniques\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print(\"===================NLTK Tokenizer===================\")\n",
    "    print(run_nltk_tokenizer(sentence))\n",
    "    print(\"===================NLTK Word NGRAM Tokenizer 2 words===================\")\n",
    "    print(run_nltk_tokenizer_word_ngrams(sentence,2))\n",
    "    print(\"===================NLTK Word NGRAM Tokenizer 3 words===================\")\n",
    "    print(run_nltk_tokenizer_word_ngrams(sentence,3))\n",
    "    print(\"===================Phrase Machine===================\")\n",
    "    #phrases=run_phrase_machine(sentence)\n",
    "    #for term in phrases[\"counts\"].keys():\n",
    "    #    print(term)\n",
    "    print(\"===================Rake===================\")\n",
    "    print(run_rake(sentence))\n",
    "    print(\"===================NLTK Tokenizer===================\")\n",
    "    print(run_nltk_tokenizer((sentence)))\n",
    "    print(\"===================NLTK Tokenizer LOWER CASE===================\")\n",
    "    print(run_nltk_tokenizer(lower_case(sentence)))\n",
    "    print(\"===================NLTK Tokenizer REMOVE STOP WORDS===================\")\n",
    "    print(remove_stop_words(sentence))   \n",
    "    print(\"===================NLTK Tokenizer REMOVED PUNCTUATION===================\")\n",
    "    print(run_nltk_tokenizer(remove_punctuation(sentence)))\n",
    "    print(\"===================NLTK Tokenizer REMOVED TAGS===================\")\n",
    "    print(run_nltk_tokenizer(remove_tags(sentence)))\n",
    "    print(\"===================NLTK Tokenizer REMOVED CHARS AND DIGITS===================\")\n",
    "    print(run_nltk_tokenizer(remove_special_chars_and_digits(sentence)))\n",
    "    print(\"===================NLTK Tokenizer STEMMING APPLIED===================\")\n",
    "    print(run_nltk_tokenizer(apply_stemming(sentence)))\n",
    "    print(\"===================NLTK Tokenizer LEMMATIZATION APPLIED===================\")\n",
    "    print(run_nltk_tokenizer(apply_lemmatization(sentence)))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348f473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
